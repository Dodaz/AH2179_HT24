{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dodaz/AH2179_HT24/blob/main/AH2179_EX5_Algot_Sjoholm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAlPf9WW9D9O"
      },
      "source": [
        "# Revealing representative \"typical\" day-types using traffic data observation and clustering\n",
        "\n",
        "This exercise can be divided to two parts. Both give you the toolsets of methods for exploring, visualazing and evaluating day clusterings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IWUHGQ-quPi"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIbPBmGqxh00"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#upload the dataset by downloading both datasets from canvas and upload it on colab\n",
        "\n",
        "data_df = pd.read_csv(\"dataset_exercise_5_clustering_highway_traffic.csv\",sep=\";\")\n",
        "data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGCheycS96cI"
      },
      "source": [
        "Your objective here is to reveal representative day-type clusters; hence, we cluster days. The provided dataset is 5-minute observations on highway microwave sensors and needs to be in a format ready for day clustering. Thus, we represent days as ordered vectors of day-time observations of size 288 (there are 288, 5-minute time observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IYSKJGoy4T4"
      },
      "outputs": [],
      "source": [
        "# Sort the DataFrame 'data_df' by columns \"Date\" and \"Interval_5\"\n",
        "data_df.sort_values([\"Date\", \"Interval_5\"])\n",
        "\n",
        "# Extract unique dates from the sorted DataFrame\n",
        "days = np.unique(data_df[['Date']].values.ravel())\n",
        "# Calculate the total number of unique days\n",
        "ndays = len(days)\n",
        "\n",
        "# Group the DataFrame 'data_df' by the \"Date\" column\n",
        "day_subsets_df = data_df.groupby([\"Date\"])\n",
        "\n",
        "# Define the total number of 5-minute intervals in a day\n",
        "nintvals = 288\n",
        "\n",
        "# Create a matrix 'vectorized_day_dataset' filled with NaN values\n",
        "vectorized_day_dataset = np.zeros((ndays, nintvals))\n",
        "vectorized_day_dataset.fill(np.nan)\n",
        "\n",
        "# Loop through each unique day\n",
        "for i in range(0, ndays):\n",
        "    # Get the DataFrame corresponding to the current day\n",
        "    df_t = day_subsets_df.get_group(days[i])\n",
        "\n",
        "    # Loop through each row in the current day's DataFrame\n",
        "    for j in range(len(df_t)):\n",
        "        # Get the current day's DataFrame\n",
        "        df_t = day_subsets_df.get_group(days[i])\n",
        "\n",
        "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset'\n",
        "        vectorized_day_dataset[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
        "\n",
        "# Print the resulting 'vectorized_day_dataset'\n",
        "print(vectorized_day_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6w2V4_X_Jte"
      },
      "source": [
        "* STEP 1: We ensure the ordering of days and intervals is proper. The data integer representation and indexing of time intervals allow a simple order of data.\n",
        "* STEP 2: We get all unique days in the dataset to know the number and their list\n",
        "* STEP 3: We could do the same of time-intervals, but here we know it is 288\n",
        "* STEP 4: We create new datasets where rows are days and columns are day-time interval observations. *Note that if you plan to reveal network-wide day-types, this day vector can be ordered vector of SENSORS * TIME-INTERVALS.*\n",
        "* STEP 5: fill in data to the right indexes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4xan1q3Ai-v"
      },
      "source": [
        "# Part 1: Data exploration\n",
        "\n",
        "It is always good practice to explore data you work with for outliers or pattern existence, as it can impact the revealed patterns and their predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCycuUGJXajK"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('number of nans',np.sum(np.isnan(vectorized_day_dataset)))\n",
        "print('rate of nans',np.sum(np.isnan(vectorized_day_dataset))/(ndays*nintvals))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiCP8S1bAyhS"
      },
      "source": [
        "In the dataset, we have only missing 277 values, which is 0.26%. Let us check its distribution in the dataset. First, missing data could be related to night hours, with a higher possibility of no vehicle observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz1oAfKR_LM0"
      },
      "outputs": [],
      "source": [
        "nans_per_time = np.sum(np.isnan(vectorized_day_dataset),0)\n",
        "print(nans_per_time.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# Create an array 'x_axis' representing the 5-minute intervals\n",
        "x_axis = np.arange(0, nintvals, 1, dtype=int)\n",
        "# Initialize an empty list 'x_axis_hours' to store time values in hours\n",
        "x_axis_hours = []\n",
        "# Convert interval indices to hours and append them to 'x_axis_hours'\n",
        "for i in range(0, len(x_axis)):\n",
        "  x_axis_hours.append(float(x_axis[i]*5)/60)\n",
        "ax.bar(x_axis_hours,height=nans_per_time)\n",
        "\n",
        "\n",
        "ax.set_ylabel('number of missing values')\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "ax.set_title('Time profile of missing values')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZjUIBmjBaWt"
      },
      "source": [
        "Are these missing values associated with just a few days?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TppVTgUhDvbi"
      },
      "outputs": [],
      "source": [
        "nans_per_day = np.sum(np.isnan(vectorized_day_dataset),1)\n",
        "print('number of days with missing value',np.size(np.where(nans_per_day > 0),1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jByZuZZfGCqN"
      },
      "source": [
        "What does the data look like? What is the traffic pattern? Below is the script that can help you visualize all days where overlapping transparencies highlight some patterns. The black line is the average yearly flow for a time interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2CEwLhwGEln"
      },
      "outputs": [],
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()# a convenient way to create a new figure and a set of subplots.\n",
        "ax.plot(np.array([x_axis_hours,]*ndays).transpose(),vectorized_day_dataset.transpose(),color='#444444',alpha=0.05)\n",
        "# Above line plots the dataset with specified color and transparency.\n",
        "ax.plot(x_axis_hours,np.transpose(np.nanmean(vectorized_day_dataset,0)),color='black')\n",
        "# Above line plots the average of the dataset in black color.\n",
        "\n",
        "ax.set_ylabel('Average flow')\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "plt.xlim(0,24)\n",
        "ax.set_title('Daily profile of flow dynamic')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-7BQUzQcacL"
      },
      "outputs": [],
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()  # This line is a convenient way to create a new figure and a set of subplots.\n",
        "\n",
        "# Create a boxplot for the dataset\n",
        "boxplot = ax.boxplot(vectorized_day_dataset.T, patch_artist=True)\n",
        "\n",
        "# Customize the boxplot appearance\n",
        "for patch in boxplot['boxes']:\n",
        "    patch.set_facecolor('#444444')  # Set the box color to gray\n",
        "for median in boxplot['medians']:\n",
        "    median.set(color='black', linewidth=2)  # Set median line color to black\n",
        "\n",
        "# Set the y-axis label\n",
        "ax.set_ylabel('Flow')\n",
        "\n",
        "# Set the x-axis label\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "\n",
        "# Set the x-axis limits to be between 0 and 24\n",
        "plt.xlim(0, 24)\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Daily Profile of Flow Dynamics (Boxplot)')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XECofu54LbHT"
      },
      "source": [
        "Some patterns are obvious, and we could expect some peak and weekend patterns. Thus, a simple thing to do is look at day-of-week patterns, done below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiYId1bLLeBr"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# Create an array 'day_of_week' to store the day of the week for each unique date\n",
        "day_of_week = np.zeros((ndays))\n",
        "\n",
        "# Loop through each unique date\n",
        "for i in range(0, ndays):\n",
        "    # Parse the current date from a string to a datetime object\n",
        "    day_dt = datetime.datetime.strptime(str(days[i]), '%Y%m%d')\n",
        "\n",
        "    # Get the day of the week (1 for Monday, 2 for Tuesday, ..., 7 for Sunday)\n",
        "    day_of_week[i] = day_dt.isoweekday()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UedwK90ZGdZx"
      },
      "outputs": [],
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Iterate through each day of the week (from 1 to 7)\n",
        "for i in range(1, 8):\n",
        "    # Find the indices of days that correspond to the current day of the week\n",
        "    day_of_week_index_t = np.where(day_of_week == i)\n",
        "\n",
        "    # Calculate the number of days that match the current day of the week\n",
        "    ndays_t = np.size(day_of_week_index_t[0])\n",
        "\n",
        "    # Plot the average flow for the current day of the week\n",
        "    ax.plot(x_axis_hours,\n",
        "            np.nanmean(vectorized_day_dataset[day_of_week_index_t[0], :].transpose(), 1),\n",
        "            label='day-of-week ' + str(i))\n",
        "    # This line plots the average flow for the current day of the week.\n",
        "    # 'np.nanmean()' calculates the mean while handling NaN values.\n",
        "\n",
        "# Set the y-axis label\n",
        "ax.set_ylabel('Average flow')\n",
        "\n",
        "# Set the x-axis label\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "\n",
        "# Set the x-axis limits to be between 0 and 24\n",
        "plt.xlim(0, 24)\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Daily Profile of Flow Dynamics')\n",
        "\n",
        "# Add a legend indicating the day of the week\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqtvIDWYCWqq"
      },
      "source": [
        "The day's index is from 1 - 7, where 1 is Monday. To our expectations, we can see the difference between weekdays and weekends. However, some patterns from the previous plot are missing here and may include seasonal, holiday, and incident impacts on observations. This we explore with clustering in the next part of the exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jaw2R9SrfkJ"
      },
      "source": [
        "# Part 2: Clustering\n",
        "\n",
        "In this part, you will work on revealing patterns using traffic observations and clustering methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON7OnUwzT-ku"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hagyMxjjDWmQ"
      },
      "source": [
        "Using clustering methods in scikit-learn is relatively simple, as shown below. With one line of code, you can get some clusters. However, this will need some work to search for representative clusters. This is the first step to pattern exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV_T3yuOr0b9"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "#clusters = KMeans(n_clusters=10, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy3KGTezDxo_"
      },
      "source": [
        "As you can see k-means clustering method can not handle missing values, so you have choices: impute data or remove them. We will just remove all days that have missing observations. Fewer days would be removed if we restrict the clustering to a particular day-time period, 06:00 - 22:00:00. Below, we prepare a new dataset without missing values and update the list of days for later visualization purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOfS0A8Xt6mh"
      },
      "outputs": [],
      "source": [
        "n_clusters = 2\n",
        "clusters = None\n",
        "#print(np.where(nans_per_day > 0)[0])\n",
        "vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
        "days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
        "\n",
        "# BELOW lines enables you to comment in and out clustering method you want to use note that GMM have different ouput and thus labels are extracted differently\n",
        "clusters = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "# clusters = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
        "# clusters = DBSCAN(eps=500, min_samples = 2).fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
        "\n",
        "if clusters is not None:\n",
        "  cluster_labels = clusters.labels_\n",
        "\n",
        "#cluster_labels = GaussianMixture(n_components=n_clusters).fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans) #check the parameters at  https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_init.html#sphx-glr-auto-examples-mixture-plot-gmm-init-py\n",
        "\n",
        "\n",
        "print(cluster_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nv6CseOTz_g"
      },
      "source": [
        "## Visualizaiton of representative day-type patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6Dr8JQ0USJk"
      },
      "source": [
        "### Special plots for visualizing day-type patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXrQhfpPujQS"
      },
      "source": [
        "the results of clusterings are assignments to the clusters, this can be hard to read and make conclusions about it, so visualization of data in right way is of high importance. Below script gives you set of libraries for calendar and centroid visualizaiton.\n",
        "\n",
        "***Note: The below script you do not have to understand. Consider it as an external library that will plot for you, just like a histogram plot, for which you also do not know the exact implementation. Anyway, this course does not focus on information visualization.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9A7NgRaul7A"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from matplotlib.patches import Polygon\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib import gridspec\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib import colors\n",
        "\n",
        "def assign_colors(n_clusters, days, assigments):\n",
        "\n",
        "    days_colors = []\n",
        "    color_to_cluster = []\n",
        "    style_to_cluster = []\n",
        "    weekend_colors = ['#67001f','#d6604d','#fdae61','#f46d43','#d53e4f','#9e0142','#f768a1','#f1c232']#,'#fe9929','#cc4c02','#e31a1c','#737373','#bdbdbd','#252525','#bcbddc']\n",
        "#    weekend_school_colors = ['#c2a5cf','#f1b6da','#8e0152','#c51b7d','#de77ae','#ae017e','#fcc5c0','#e31a1c','#737373','#bdbdbd']\n",
        "#    bank_holidays_colors = ['#543005','#dfc27d','#bf812d','#8c510a']\n",
        "    mixed_colors = ['#4d4d4d','#35978f','#bababa','#878787']\n",
        "    weekday_colors = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#cab2d6','#6a3d9a','#b15928','#8dd3c7','#bebada','#fb8072','#b3de69','#bc80bd','#fccde5','#ccebc5','#35978f','#80cdc1']\n",
        "\n",
        "    cluster_id_weekdays_share = []\n",
        "    cluster_id_weekend_share = []\n",
        "    cluster_id_all_days = []\n",
        "\n",
        "    for i in range(0,n_clusters):\n",
        "        color_to_cluster.append(None)\n",
        "        style_to_cluster.append(None)\n",
        "        cluster_id_weekdays_share.append(0)\n",
        "        cluster_id_weekend_share.append(0)\n",
        "        cluster_id_all_days.append(0)\n",
        "\n",
        "    for i in range(0,len(days)):\n",
        "        #print(i,assigments[i],len(assigments),len(cluster_id_all_days))\n",
        "        if assigments[i] is not None:\n",
        "            cluster_id_all_days[assigments[i]] += 1\n",
        "            if '-' in str(days[i]):\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
        "            else:\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
        "\n",
        "            if int(pomT.weekday()) < 5:\n",
        "                cluster_id_weekdays_share[assigments[i]] += 1\n",
        "            else:\n",
        "                cluster_id_weekend_share[assigments[i]] += 1\n",
        "\n",
        "    print('cluster_id_weekdays_share',cluster_id_weekdays_share)\n",
        "    print('cluster_id_weekend_share',cluster_id_weekend_share)\n",
        "    for i in range(0,len(days)):\n",
        "        if assigments[i] is not None:\n",
        "            cluster_idx = assigments[i]\n",
        "            if '-' in str(days[i]):\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
        "            else:\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
        "            if color_to_cluster[assigments[i]] is None:\n",
        "                if cluster_id_weekend_share[cluster_idx] / float(cluster_id_all_days[cluster_idx]) > 0.6:\n",
        "                        color_to_cluster[assigments[i]] = weekend_colors.pop()\n",
        "                        style_to_cluster[assigments[i]] = ':'\n",
        "                elif cluster_id_weekdays_share[cluster_idx] / float(cluster_id_all_days[cluster_idx]) > 0.6:\n",
        "                        color_to_cluster[assigments[i]] = weekday_colors.pop(0)\n",
        "                        style_to_cluster[assigments[i]] = '-'\n",
        "                else:\n",
        "                    color_to_cluster[assigments[i]] = mixed_colors.pop()\n",
        "                    style_to_cluster[assigments[i]] = ':'\n",
        "\n",
        "            days_colors.append(color_to_cluster[assigments[i]])\n",
        "        else:\n",
        "            days_colors.append(None)\n",
        "\n",
        "    return days_colors,color_to_cluster,style_to_cluster\n",
        "\n",
        "\n",
        "def calmap(ax, year, data, days, assigments, n_clusters,days_colors,color_to_cluster,\n",
        "           limit_graphics=False):\n",
        "\n",
        "    ax.tick_params('x', length=0, labelsize=\"medium\", which='major')\n",
        "    ax.tick_params('y', length=0, labelsize=\"x-small\", which='major')\n",
        "\n",
        "    # Month borders\n",
        "\n",
        "    xticks, labels = [], []\n",
        "    start = datetime.datetime(year,1,1).weekday()\n",
        "\n",
        "    for month in range(1,13):\n",
        "\n",
        "        first = datetime.datetime(year, month, 1)\n",
        "        last = first + relativedelta(months=1, days=-1)\n",
        "\n",
        "        y0 = first.weekday()\n",
        "        y1 = last.weekday()\n",
        "        x0 = (int(first.strftime(\"%j\"))+start-1)//7\n",
        "        x1 = (int(last.strftime(\"%j\"))+start-1)//7\n",
        "\n",
        "        P = [ (x0,   y0), (x0,    7),  (x1,   7),\n",
        "              (x1,   y1+1), (x1+1,  y1+1), (x1+1, 0),\n",
        "              (x0+1,  0), (x0+1,  y0) ]\n",
        "\n",
        "        xticks.append(x0 +(x1-x0+1)/2)\n",
        "        labels.append(first.strftime(\"%b\"))\n",
        "        poly = Polygon(P, edgecolor=\"black\", facecolor=\"None\",\n",
        "\n",
        "                       linewidth=1, zorder=20, clip_on=False)\n",
        "\n",
        "        ax.add_artist(poly)\n",
        "\n",
        "    line = Line2D([0,53],[5,5],linewidth=1, zorder = 20,color=\"black\",linestyle='dashed')\n",
        "    ax.add_artist(line)\n",
        "\n",
        "    if not limit_graphics:\n",
        "        ax.set_xticks(xticks)\n",
        "        ax.set_xticklabels(labels)\n",
        "        ax.set_yticks(0.5 + np.arange(7))\n",
        "        ax.set_yticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n",
        "        ax.set_title(\"{}\".format(year), weight=\"semibold\")\n",
        "    else:\n",
        "        plt.tick_params(\n",
        "            axis='x',          # changes apply to the x-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            bottom=False,      # ticks along the bottom edge are off\n",
        "            top=False,         # ticks along the top edge are off\n",
        "            labelbottom=False)\n",
        "        plt.tick_params(\n",
        "            axis='y',          # changes apply to the x-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            left=False,      # ticks along the bottom edge are off\n",
        "            right=False,         # ticks along the top edge are off\n",
        "            labelleft=False)\n",
        "\n",
        "    # Clearing first and last day from the data\n",
        "    valid = datetime.datetime(year, 1, 1).weekday()\n",
        "    data[:valid,0] = np.nan\n",
        "    valid = datetime.datetime(year, 12, 31).weekday()\n",
        "    # data[:,x1+1:] = np.nan\n",
        "    data[valid+1:,x1] = np.nan\n",
        "\n",
        "    for i in range(0,len(days)):\n",
        "        if '-' in str(days[i]):\n",
        "            pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
        "        else:\n",
        "            pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
        "        week_number = int(pomT.strftime(\"%W\"))\n",
        "        day_of_week = int(pomT.weekday())\n",
        "        data[day_of_week,week_number] = assigments[i]\n",
        "\n",
        "\n",
        "    act_date = datetime.datetime(year,1,1)\n",
        "    while (act_date.year == year):\n",
        "\n",
        "        week_number = int(act_date.strftime(\"%W\"))\n",
        "        day_of_week = int(act_date.weekday())\n",
        "        doy_id = act_date.timetuple().tm_yday\n",
        "        if doy_id<5 and week_number > 53:\n",
        "            week_number = 0\n",
        "\n",
        "        act_date = act_date + datetime.timedelta(days=1)\n",
        "\n",
        "    #pomT = datetime.datetime.strptime('2017-01-01','%Y-%m-%d')\n",
        "    #week_number = int(pomT.strftime(\"%V\"))\n",
        "    #day_of_week = int(pomT.weekday())\n",
        "    #print(week_number,day_of_week)\n",
        "    #doy_id = pomT.timetuple().tm_yday\n",
        "    #if doy_id<5 and week_number > 0:\n",
        "    #    week_number = 0\n",
        "    #data[day_of_week,week_number] = len(clusters)+10\n",
        "\n",
        "    # Showing data\n",
        "    cmap = plt.cm.spring  # Can be any colormap that you want after the cm\n",
        "    cmap.set_bad(color='white')\n",
        "\n",
        "    #ax.imshow(data, extent=[0,53,0,7], zorder=10, vmin=0, vmax=len(clusters)+10,\n",
        "    #          cmap=cmap, origin=\"lower\", alpha=.75)\n",
        "\n",
        "    cmap = colors.ListedColormap(color_to_cluster)\n",
        "    bounds=[-0.1]\n",
        "    step = 1\n",
        "    for i in range(0,n_clusters):\n",
        "        bounds.append(i-0.1+step)\n",
        "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
        "    #print(color_to_cluster)\n",
        "   #print(bounds)\n",
        "    #print(norm)\n",
        "\n",
        "    #print(color_to_cluster)\n",
        "    #print(bounds)\n",
        "    #print(cmap)\n",
        "    #exit(0)\n",
        "\n",
        "    ax.imshow(data, extent=[0,53,0,7], zorder=10, interpolation='nearest', origin='lower',cmap=cmap, norm=norm)\n",
        "\n",
        "def make_calendar_visualization_figure(days,assigments,n_clusters,years,days_colors,color_to_cluster,\n",
        "                                       save_figure: str = None, show_figure:bool = True, limit_graphics = False):\n",
        "\n",
        "    fig = plt.figure(figsize=(8,1.5*len(years)), dpi=100)\n",
        "    X = np.linspace(-1,1, 53*7)\n",
        "\n",
        "    for i, obj in enumerate(years):\n",
        "\n",
        "        pom_s = str(len(years))+'1'+str(i+1)\n",
        "        print(pom_s)\n",
        "\n",
        "        ax = plt.subplot(int(pom_s), xlim=[0, 53], ylim=[0, 7], frameon=False, aspect=1)\n",
        "        I = 1.2 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
        "        I = I.reshape(53,7).T\n",
        "        I.fill(np.nan)\n",
        "        calmap(ax, int(obj), I.reshape(53,7).T, days, assigments, n_clusters,days_colors,color_to_cluster, limit_graphics)\n",
        "\n",
        "    #   ax = plt.subplot(212, xlim=[0,53], ylim=[0,7], frameon=False, aspect=1)\n",
        "    #  I = 1.1 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
        "    #   calmap(ax, 2018, I.reshape(53,7).T)\n",
        "\n",
        "    #ax = plt.subplot(313, xlim=[0,53], ylim=[0,7], frameon=False, aspect=1)\n",
        "    #I = 1.0 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
        "    #calmap(ax, 2019, I.reshape(53,7).T)\n",
        "    if save_figure:\n",
        "        plt.savefig(save_figure)\n",
        "\n",
        "    if show_figure or save_figure is None:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def make_figure_centroids(x,y,color_to_cluster,style_to_cluster,cluster_ids,minY = None,maxY = None,\n",
        "                          save_figure: str = None, show_figure:bool = True):\n",
        "\n",
        "    #print(color_to_cluster)\n",
        "    fig = plt.figure(figsize=(8,3))\n",
        "    ax = fig.add_subplot(111)\n",
        "    for i in range(0,len(x)):\n",
        "        #print(i,color_to_cluster[i],style_to_cluster[i])\n",
        "        #print(y[i])\n",
        "        ax.plot(x[i],y[i],style_to_cluster[i], color=color_to_cluster[i], label=str(cluster_ids[i]))\n",
        "    ax.set_xlabel('Time of day')\n",
        "    ax.set_ylabel('Flow')\n",
        "    if minY is not None and maxY is not None:\n",
        "        ax.set_ylim([minY, maxY])\n",
        "    plt.legend()\n",
        "\n",
        "    if save_figure:\n",
        "        plt.savefig(save_figure)\n",
        "\n",
        "    if show_figure or save_figure is None:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0HZj5jpUf6n"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOm8M7jPUuCl"
      },
      "source": [
        "#### Calendar visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3rx31eUEuYb"
      },
      "source": [
        "Using above functions for visualization, the representative day-type clusters can be visualize in form of calendar in order to enable for us by-eye analysis to make sense of these clusters. What is your reflection about them? Note that white cells are removed days because of missing observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPwbOdp1voEh"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of clusters by finding unique values in 'cluster_labels'\n",
        "n_clusters_t = len(np.unique(cluster_labels))\n",
        "\n",
        "# Assign colors to days based on clusters\n",
        "days_colors, color_to_cluster, style_to_cluster = assign_colors(n_clusters_t, days_not_nans, cluster_labels)\n",
        "# The function 'assign_colors' is used to determine colors and styles for visualization.\n",
        "\n",
        "# Create a calendar visualization figure\n",
        "make_calendar_visualization_figure(days_not_nans, cluster_labels, n_clusters_t, [2021], days_colors,\n",
        "                                   color_to_cluster, save_figure=None)\n",
        "# This function 'make_calendar_visualization_figure' is used to generate a visualization based on the provided data and parameters.\n",
        "# 'days_not_nans' are the days, 'cluster_labels' are the cluster labels, 'n_clusters_t' is the number of clusters,\n",
        "# '[2021]' represents the year, 'days_colors' represent the assigned colors for each day, 'color_to_cluster' maps colors to clusters,\n",
        "# and 'save_figure' is an optional parameter to save the generated figure (can be None if not saving)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V4Ln1QSU0fZ"
      },
      "source": [
        "#### Day-time profile of centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeUR_UqR4e1A"
      },
      "outputs": [],
      "source": [
        "# Initialize empty lists to store centroid data\n",
        "centroids_xx = []  # x-axis values for centroids\n",
        "centroids_yy_daytypes = []  # y-axis values for centroids, grouped by day types\n",
        "cluster_ids = []  # Cluster IDs\n",
        "\n",
        "# Iterate through each cluster\n",
        "for i in range(0, n_clusters_t):\n",
        "    # Store the x-axis values for centroids (hours of the day)\n",
        "    centroids_xx.append(x_axis_hours)\n",
        "\n",
        "    # Calculate the y-axis values for centroids (average flow for each 5-minute interval)\n",
        "    centroid_yy = list(np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).transpose())\n",
        "    centroids_yy_daytypes.append(centroid_yy)\n",
        "\n",
        "    # Store the cluster ID\n",
        "    cluster_ids.append(i)\n",
        "\n",
        "# Generate a figure displaying the centroids\n",
        "make_figure_centroids(centroids_xx, centroids_yy_daytypes, color_to_cluster, style_to_cluster, cluster_ids)\n",
        "# The function 'make_figure_centroids' is used to create a visualization of the centroids,\n",
        "# with the provided data and parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ocQl6CGIj6"
      },
      "source": [
        "Next, we can explore thei day-type profilex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1_7BaeDGOOn"
      },
      "source": [
        "As you can see with first shot using k-means we can much more day-type patterns as we may expect, it even enable to identify potential outliers like cluster 7 or possible incident/outlier day 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh806WebYlyQ"
      },
      "source": [
        "# Part 3: Independent work - clustering evaluation\n",
        "\n",
        "Find the day-type clusterings that you consider most representative. Evaluate clustering quality using internal evaluation metrics such as Silhouette score and others. You can consider short-term prediction as an external metric that imitates how useful recognized patterns are compared to newly observed days. In an assignment, motivate why you consider the method selected by you as superior.\n",
        "\n",
        "**NOTE** ***This part of exercise as well with all you have learned about clustering is the basis for your reflection in mandatory grated assignment for the MODULE 5***\n",
        "\n",
        "You can use above and below scripts for experimenting with different methods, metrics and number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReJNTPK1ZQ2U"
      },
      "source": [
        "## Internal evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3SQZh9zZmpp"
      },
      "source": [
        "Below is just example for using internal metrics in scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eAXdj_bZVAG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import relevant metrics from scikit-learn\n",
        "from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\n",
        "\n",
        "# Calculate the Silhouette Score\n",
        "SC_score = silhouette_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "# Silhouette Score measures the quality of clusters, higher values indicate better separation.\n",
        "\n",
        "# Calculate the Davies-Bouldin Score\n",
        "DB_score = davies_bouldin_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "# Davies-Bouldin Score measures the average similarity between each cluster and its most similar cluster, lower values indicate better separation.\n",
        "\n",
        "# Calculate the Calinski-Harabasz Score\n",
        "CH_score = calinski_harabasz_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "# Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance, higher values indicate better separation.\n",
        "\n",
        "# Print the computed cluster quality scores\n",
        "print('Silhouette Score:', SC_score)\n",
        "print('Davies-Bouldin Score:', DB_score)\n",
        "print('Calinski-Harabasz Score:', CH_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtggY3IXZVYO"
      },
      "source": [
        "## External evaluation with short-term prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8GM5qTBZZgt"
      },
      "source": [
        "First, lets load the evaluation dataset used of evaluating short-term prediction accuracy, vectorize it to day vectors and remove missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1dqEbTgZfnx"
      },
      "outputs": [],
      "source": [
        "# Read the evaluation dataset from a CSV file\n",
        "data_eval_df = pd.read_csv(\"evaluation_dataset_exercise_5_clustering_highway_traffic.csv\", sep=\";\")\n",
        "\n",
        "# Sort the evaluation DataFrame by columns \"Date\" and \"Interval_5\"\n",
        "data_eval_df.sort_values([\"Date\", \"Interval_5\"])\n",
        "\n",
        "# Extract unique dates from the sorted evaluation DataFrame\n",
        "days_eval = np.unique(data_eval_df[['Date']].values.ravel())\n",
        "# Calculate the total number of unique days in the evaluation dataset\n",
        "ndays_eval = len(days_eval)\n",
        "\n",
        "# Group the evaluation DataFrame by the \"Date\" column\n",
        "day_eval_subsets_df = data_eval_df.groupby([\"Date\"])\n",
        "\n",
        "# Initialize a matrix 'vectorized_day_dataset_eval' filled with NaN values\n",
        "vectorized_day_dataset_eval = np.zeros((ndays_eval, nintvals))\n",
        "vectorized_day_dataset_eval.fill(np.nan)\n",
        "# This section initializes a 2D array to store the evaluation dataset and fills it with NaN values.\n",
        "\n",
        "# Loop through each unique day in the evaluation dataset\n",
        "for i in range(0, ndays_eval):\n",
        "    # Get the DataFrame corresponding to the current day\n",
        "    df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
        "\n",
        "    # Loop through each row in the current day's DataFrame\n",
        "    for j in range(len(df_t)):\n",
        "        # Get the current day's DataFrame (this line is redundant)\n",
        "        df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
        "\n",
        "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset_eval'\n",
        "        vectorized_day_dataset_eval[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
        "\n",
        "# Print the resulting 'vectorized_day_dataset_eval'\n",
        "print(vectorized_day_dataset_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNUDJ57Uj7JZ"
      },
      "outputs": [],
      "source": [
        "# Calculate the total number of NaN values in the evaluation dataset\n",
        "print('Number of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)))\n",
        "\n",
        "# Calculate the rate of NaN values in the evaluation dataset\n",
        "print('Rate of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)) / (ndays_eval * nintvals))\n",
        "\n",
        "# Calculate the number of days with missing values\n",
        "nans_per_day_eval = np.sum(np.isnan(vectorized_day_dataset_eval), 1)\n",
        "print('Number of days with missing values:', np.size(np.where(nans_per_day_eval > 0)))\n",
        "\n",
        "# Filter out days with no missing values and create a new dataset\n",
        "vectorized_day_dataset_no_nans_eval = vectorized_day_dataset_eval[np.where(nans_per_day_eval == 0)[0], :]\n",
        "days_not_nans_eval = days_eval[np.where(nans_per_day_eval == 0)[0]]\n",
        "\n",
        "# Calculate the final number of days in the evaluation dataset after removing missing values\n",
        "print('Final number of days in evaluation dataset:', len(days_not_nans_eval))\n",
        "\n",
        "# Print the list of days in the evaluation dataset with no missing values\n",
        "print('List of days without missing values:', days_not_nans_eval)\n",
        "\n",
        "# Calculate the total number of days in the filtered evaluation dataset\n",
        "ndays_eval_not_nans = len(days_not_nans_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Yc2vg9gUrJ"
      },
      "source": [
        "Now when the dataset for evaluation is ready, below is a script for evaluating short-term prediction performance.\n",
        "\n",
        "Prediction works like this for each day at current interval *j*; we use 5 last past intervals to find the closest centroid. This average centroid is used as the source of the prediction for future time interval *j+1*. This directly measures how the representative pattern matches the new future days not part of your training and thus evaluates the clustering to external data and not as internal metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6259sc-sgVE8"
      },
      "outputs": [],
      "source": [
        "# Import the pairwise_distances function from scikit-learn's metrics library\n",
        "import sklearn.metrics.pairwise as dis_lib\n",
        "\n",
        "# Define a function to find the closest centroid to a new data point within a specified day-time interval range\n",
        "def find_the_closest_centroid(centroids, new_day, from_interval: int, to_interval: int):\n",
        "    closest_centroid = None\n",
        "    closest_dist = None\n",
        "\n",
        "    # Iterate through each centroid\n",
        "    for i in range(0, len(centroids)):\n",
        "        # Calculate the Euclidean distance between the centroid and the new data point\n",
        "        ed_t = dis_lib.paired_distances(centroids[i], new_day, metric='euclidean')\n",
        "\n",
        "        # Check if the current centroid is closer than the previously closest one\n",
        "        if closest_centroid is None or closest_dist > ed_t:\n",
        "            closest_centroid = i\n",
        "            closest_dist = ed_t\n",
        "\n",
        "    return closest_centroid\n",
        "\n",
        "# Initialize a list to store centroid data\n",
        "centroids = []\n",
        "\n",
        "# Calculate centroids for each cluster\n",
        "for i in np.unique(cluster_labels):\n",
        "    centroid = np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).reshape(1, nintvals)\n",
        "    centroids.append(centroid)\n",
        "\n",
        "# Define the number of past intervals to consider for classification\n",
        "n_past_intervals_for_classification = 5\n",
        "\n",
        "# Initialize variables to calculate accuracy metrics\n",
        "total_mae = 0\n",
        "total_mape = 0\n",
        "prediction_counts = 0\n",
        "\n",
        "# Loop through each day in the evaluation dataset with no missing values\n",
        "for i in range(0, ndays_eval_not_nans):\n",
        "    # Loop through intervals from n_past_intervals_for_classification to nintvals - 1\n",
        "    for j in range(n_past_intervals_for_classification, nintvals - 1):\n",
        "        # Find the closest centroid for the current data point\n",
        "        centroid_index = find_the_closest_centroid(centroids, vectorized_day_dataset_no_nans_eval[i].reshape(1, nintvals), j - n_past_intervals_for_classification, j)\n",
        "\n",
        "        # Predict the value for the next interval\n",
        "        predicted_value = centroids[centroid_index][0, j + 1]\n",
        "\n",
        "        # Calculate Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE)\n",
        "        mae_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "        mape_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1]) / float(vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "\n",
        "        # Accumulate MAE, MAPE, and count of predictions\n",
        "        total_mae += mae_t\n",
        "        total_mape += mape_t\n",
        "        prediction_counts += 1\n",
        "\n",
        "# Calculate and print the prediction accuracy metrics\n",
        "print('Prediction accuracy MAE:', total_mae / prediction_counts)\n",
        "print('Prediction accuracy MAPE:', total_mape / prediction_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Task"
      ],
      "metadata": {
        "id": "FCJcUsEfVamd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "print('KMeans')\n",
        "for n_clusters in range(2, 11):\n",
        "  clusters = None\n",
        "  #print(np.where(nans_per_day > 0)[0])\n",
        "  vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
        "  days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
        "\n",
        "  # BELOW lines enables you to comment in and out clustering method you want to use note that GMM have different ouput and thus labels are extracted differently\n",
        "  clusters = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "  # clusters = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
        "  # clusters = DBSCAN(eps=500, min_samples = 2).fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
        "\n",
        "  if clusters is not None:\n",
        "    cluster_labels = clusters.labels_\n",
        "\n",
        "  #cluster_labels = GaussianMixture(n_components=n_clusters).fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans) #check the parameters at  https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_init.html#sphx-glr-auto-examples-mixture-plot-gmm-init-py\n",
        "\n",
        "  # Calculate the number of clusters by finding unique values in 'cluster_labels'\n",
        "  n_clusters_t = len(np.unique(cluster_labels))\n",
        "\n",
        "  # Assign colors to days based on clusters\n",
        "  days_colors, color_to_cluster, style_to_cluster = assign_colors(n_clusters_t, days_not_nans, cluster_labels)\n",
        "  # The function 'assign_colors' is used to determine colors and styles for visualization.\n",
        "\n",
        "  # Create a calendar visualization figure\n",
        "  make_calendar_visualization_figure(days_not_nans, cluster_labels, n_clusters_t, [2021], days_colors,\n",
        "                                    color_to_cluster, save_figure=None)\n",
        "  # This function 'make_calendar_visualization_figure' is used to generate a visualization based on the provided data and parameters.\n",
        "  # 'days_not_nans' are the days, 'cluster_labels' are the cluster labels, 'n_clusters_t' is the number of clusters,\n",
        "  # '[2021]' represents the year, 'days_colors' represent the assigned colors for each day, 'color_to_cluster' maps colors to clusters,\n",
        "  # and 'save_figure' is an optional parameter to save the generated figure (can be None if not saving).\n",
        "\n",
        "  # Initialize empty lists to store centroid data\n",
        "  centroids_xx = []  # x-axis values for centroids\n",
        "  centroids_yy_daytypes = []  # y-axis values for centroids, grouped by day types\n",
        "  cluster_ids = []  # Cluster IDs\n",
        "\n",
        "  # Iterate through each cluster\n",
        "  for i in range(0, n_clusters_t):\n",
        "      # Store the x-axis values for centroids (hours of the day)\n",
        "      centroids_xx.append(x_axis_hours)\n",
        "\n",
        "      # Calculate the y-axis values for centroids (average flow for each 5-minute interval)\n",
        "      centroid_yy = list(np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).transpose())\n",
        "      centroids_yy_daytypes.append(centroid_yy)\n",
        "\n",
        "      # Store the cluster ID\n",
        "      cluster_ids.append(i)\n",
        "\n",
        "  # Generate a figure displaying the centroids\n",
        "  make_figure_centroids(centroids_xx, centroids_yy_daytypes, color_to_cluster, style_to_cluster, cluster_ids)\n",
        "  # The function 'make_figure_centroids' is used to create a visualization of the centroids,\n",
        "  # with the provided data and parameters.\n",
        "\n",
        "  # Import relevant metrics from scikit-learn\n",
        "  from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\n",
        "\n",
        "  # Calculate the Silhouette Score\n",
        "  SC_score = silhouette_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Silhouette Score measures the quality of clusters, higher values indicate better separation.\n",
        "\n",
        "  # Calculate the Davies-Bouldin Score\n",
        "  DB_score = davies_bouldin_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Davies-Bouldin Score measures the average similarity between each cluster and its most similar cluster, lower values indicate better separation.\n",
        "\n",
        "  # Calculate the Calinski-Harabasz Score\n",
        "  CH_score = calinski_harabasz_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance, higher values indicate better separation.\n",
        "\n",
        "  # Print the computed cluster quality scores\n",
        "  print('Number of Clusters: ', n_clusters)\n",
        "  print('Silhouette Score:', SC_score)\n",
        "  print('Davies-Bouldin Score:', DB_score)\n",
        "  print('Calinski-Harabasz Score:', CH_score)\n",
        "  print('')\n",
        "\n",
        "  # Initialize a list to store centroid data\n",
        "  centroids = []\n",
        "\n",
        "  # Calculate centroids for each cluster\n",
        "  for i in np.unique(cluster_labels):\n",
        "      centroid = np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).reshape(1, nintvals)\n",
        "      centroids.append(centroid)\n",
        "\n",
        "  # Define the number of past intervals to consider for classification\n",
        "  n_past_intervals_for_classification = 5\n",
        "\n",
        "  # Initialize variables to calculate accuracy metrics\n",
        "  total_mae = 0\n",
        "  total_mape = 0\n",
        "  prediction_counts = 0\n",
        "\n",
        "  # Loop through each day in the evaluation dataset with no missing values\n",
        "  for i in range(0, ndays_eval_not_nans):\n",
        "      # Loop through intervals from n_past_intervals_for_classification to nintvals - 1\n",
        "      for j in range(n_past_intervals_for_classification, nintvals - 1):\n",
        "          # Find the closest centroid for the current data point\n",
        "          centroid_index = find_the_closest_centroid(centroids, vectorized_day_dataset_no_nans_eval[i].reshape(1, nintvals), j - n_past_intervals_for_classification, j)\n",
        "\n",
        "          # Predict the value for the next interval\n",
        "          predicted_value = centroids[centroid_index][0, j + 1]\n",
        "\n",
        "          # Calculate Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE)\n",
        "          mae_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "          mape_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1]) / float(vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "\n",
        "          # Accumulate MAE, MAPE, and count of predictions\n",
        "          total_mae += mae_t\n",
        "          total_mape += mape_t\n",
        "          prediction_counts += 1\n",
        "\n",
        "  # Calculate and print the prediction accuracy metrics\n",
        "  print('Prediction accuracy MAE:', total_mae / prediction_counts)\n",
        "  print('Prediction accuracy MAPE:', total_mape / prediction_counts)\n"
      ],
      "metadata": {
        "id": "ANtRe--5VZ0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "print('AgglomerativeClustering')\n",
        "for n_clusters in range(2, 11):\n",
        "  clusters = None\n",
        "  #print(np.where(nans_per_day > 0)[0])\n",
        "  vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
        "  days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
        "\n",
        "  # BELOW lines enables you to comment in and out clustering method you want to use note that GMM have different ouput and thus labels are extracted differently\n",
        "  #clusters = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "  clusters = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
        "  # clusters = DBSCAN(eps=500, min_samples = 2).fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
        "\n",
        "  if clusters is not None:\n",
        "    cluster_labels = clusters.labels_\n",
        "\n",
        "  #cluster_labels = GaussianMixture(n_components=n_clusters).fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans) #check the parameters at  https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_init.html#sphx-glr-auto-examples-mixture-plot-gmm-init-py\n",
        "\n",
        "  # Calculate the number of clusters by finding unique values in 'cluster_labels'\n",
        "  n_clusters_t = len(np.unique(cluster_labels))\n",
        "\n",
        "  # Assign colors to days based on clusters\n",
        "  days_colors, color_to_cluster, style_to_cluster = assign_colors(n_clusters_t, days_not_nans, cluster_labels)\n",
        "  # The function 'assign_colors' is used to determine colors and styles for visualization.\n",
        "\n",
        "  # Create a calendar visualization figure\n",
        "  make_calendar_visualization_figure(days_not_nans, cluster_labels, n_clusters_t, [2021], days_colors,\n",
        "                                    color_to_cluster, save_figure=None)\n",
        "  # This function 'make_calendar_visualization_figure' is used to generate a visualization based on the provided data and parameters.\n",
        "  # 'days_not_nans' are the days, 'cluster_labels' are the cluster labels, 'n_clusters_t' is the number of clusters,\n",
        "  # '[2021]' represents the year, 'days_colors' represent the assigned colors for each day, 'color_to_cluster' maps colors to clusters,\n",
        "  # and 'save_figure' is an optional parameter to save the generated figure (can be None if not saving).\n",
        "\n",
        "  # Initialize empty lists to store centroid data\n",
        "  centroids_xx = []  # x-axis values for centroids\n",
        "  centroids_yy_daytypes = []  # y-axis values for centroids, grouped by day types\n",
        "  cluster_ids = []  # Cluster IDs\n",
        "\n",
        "  # Iterate through each cluster\n",
        "  for i in range(0, n_clusters_t):\n",
        "      # Store the x-axis values for centroids (hours of the day)\n",
        "      centroids_xx.append(x_axis_hours)\n",
        "\n",
        "      # Calculate the y-axis values for centroids (average flow for each 5-minute interval)\n",
        "      centroid_yy = list(np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).transpose())\n",
        "      centroids_yy_daytypes.append(centroid_yy)\n",
        "\n",
        "      # Store the cluster ID\n",
        "      cluster_ids.append(i)\n",
        "\n",
        "  # Generate a figure displaying the centroids\n",
        "  make_figure_centroids(centroids_xx, centroids_yy_daytypes, color_to_cluster, style_to_cluster, cluster_ids)\n",
        "  # The function 'make_figure_centroids' is used to create a visualization of the centroids,\n",
        "  # with the provided data and parameters.\n",
        "\n",
        "  # Import relevant metrics from scikit-learn\n",
        "  from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\n",
        "\n",
        "  # Calculate the Silhouette Score\n",
        "  SC_score = silhouette_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Silhouette Score measures the quality of clusters, higher values indicate better separation.\n",
        "\n",
        "  # Calculate the Davies-Bouldin Score\n",
        "  DB_score = davies_bouldin_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Davies-Bouldin Score measures the average similarity between each cluster and its most similar cluster, lower values indicate better separation.\n",
        "\n",
        "  # Calculate the Calinski-Harabasz Score\n",
        "  CH_score = calinski_harabasz_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance, higher values indicate better separation.\n",
        "\n",
        "  # Print the computed cluster quality scores\n",
        "  print('Number of Clusters: ', n_clusters)\n",
        "  print('Silhouette Score:', SC_score)\n",
        "  print('Davies-Bouldin Score:', DB_score)\n",
        "  print('Calinski-Harabasz Score:', CH_score)\n",
        "  print('')\n",
        "\n",
        "  # Initialize a list to store centroid data\n",
        "  centroids = []\n",
        "\n",
        "  # Calculate centroids for each cluster\n",
        "  for i in np.unique(cluster_labels):\n",
        "      centroid = np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).reshape(1, nintvals)\n",
        "      centroids.append(centroid)\n",
        "\n",
        "  # Define the number of past intervals to consider for classification\n",
        "  n_past_intervals_for_classification = 5\n",
        "\n",
        "  # Initialize variables to calculate accuracy metrics\n",
        "  total_mae = 0\n",
        "  total_mape = 0\n",
        "  prediction_counts = 0\n",
        "\n",
        "  # Loop through each day in the evaluation dataset with no missing values\n",
        "  for i in range(0, ndays_eval_not_nans):\n",
        "      # Loop through intervals from n_past_intervals_for_classification to nintvals - 1\n",
        "      for j in range(n_past_intervals_for_classification, nintvals - 1):\n",
        "          # Find the closest centroid for the current data point\n",
        "          centroid_index = find_the_closest_centroid(centroids, vectorized_day_dataset_no_nans_eval[i].reshape(1, nintvals), j - n_past_intervals_for_classification, j)\n",
        "\n",
        "          # Predict the value for the next interval\n",
        "          predicted_value = centroids[centroid_index][0, j + 1]\n",
        "\n",
        "          # Calculate Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE)\n",
        "          mae_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "          mape_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1]) / float(vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "\n",
        "          # Accumulate MAE, MAPE, and count of predictions\n",
        "          total_mae += mae_t\n",
        "          total_mape += mape_t\n",
        "          prediction_counts += 1\n",
        "\n",
        "  # Calculate and print the prediction accuracy metrics\n",
        "  print('Prediction accuracy MAE:', total_mae / prediction_counts)\n",
        "  print('Prediction accuracy MAPE:', total_mape / prediction_counts)\n",
        "  print('')"
      ],
      "metadata": {
        "id": "jtP5FS4uXXfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "print('DBSCAN')\n",
        "for min_samplest in range(2, 15):\n",
        "  clusters = None\n",
        "  #print(np.where(nans_per_day > 0)[0])\n",
        "  vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
        "  days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
        "\n",
        "  # BELOW lines enables you to comment in and out clustering method you want to use note that GMM have different ouput and thus labels are extracted differently\n",
        "  #clusters = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "  # clusters = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
        "  clusters = DBSCAN(eps=500, min_samples = min_samplest).fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
        "\n",
        "  if clusters is not None:\n",
        "    cluster_labels = clusters.labels_\n",
        "\n",
        "  #cluster_labels = GaussianMixture(n_components=n_clusters).fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans) #check the parameters at  https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_init.html#sphx-glr-auto-examples-mixture-plot-gmm-init-py\n",
        "\n",
        "  # Import relevant metrics from scikit-learn\n",
        "  from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\n",
        "\n",
        "  # Calculate the Silhouette Score\n",
        "  SC_score = silhouette_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Silhouette Score measures the quality of clusters, higher values indicate better separation.\n",
        "\n",
        "  # Calculate the Davies-Bouldin Score\n",
        "  DB_score = davies_bouldin_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Davies-Bouldin Score measures the average similarity between each cluster and its most similar cluster, lower values indicate better separation.\n",
        "\n",
        "  # Calculate the Calinski-Harabasz Score\n",
        "  CH_score = calinski_harabasz_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance, higher values indicate better separation.\n",
        "\n",
        "  # Print the computed cluster quality scores\n",
        "  print('Min Samples: ', min_samplest)\n",
        "  print('Silhouette Score:', SC_score)\n",
        "  print('Davies-Bouldin Score:', DB_score)\n",
        "  print('Calinski-Harabasz Score:', CH_score)\n",
        "  print('')"
      ],
      "metadata": {
        "id": "IXG_zE7LYE5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "print('GaussianMixture')\n",
        "for n_clusters in range(2, 11):\n",
        "  clusters = None\n",
        "  #print(np.where(nans_per_day > 0)[0])\n",
        "  vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
        "  days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
        "\n",
        "  # BELOW lines enables you to comment in and out clustering method you want to use note that GMM have different ouput and thus labels are extracted differently\n",
        "  #clusters = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "  # clusters = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
        "  # clusters = DBSCAN(eps=500, min_samples = 2).fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
        "\n",
        "  if clusters is not None:\n",
        "    cluster_labels = clusters.labels_\n",
        "\n",
        "  cluster_labels = GaussianMixture(n_components=n_clusters).fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans) #check the parameters at  https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_init.html#sphx-glr-auto-examples-mixture-plot-gmm-init-py\n",
        "\n",
        "  # Calculate the number of clusters by finding unique values in 'cluster_labels'\n",
        "  n_clusters_t = len(np.unique(cluster_labels))\n",
        "\n",
        "  # Assign colors to days based on clusters\n",
        "  days_colors, color_to_cluster, style_to_cluster = assign_colors(n_clusters_t, days_not_nans, cluster_labels)\n",
        "  # The function 'assign_colors' is used to determine colors and styles for visualization.\n",
        "\n",
        "  # Create a calendar visualization figure\n",
        "  make_calendar_visualization_figure(days_not_nans, cluster_labels, n_clusters_t, [2021], days_colors,\n",
        "                                    color_to_cluster, save_figure=None)\n",
        "  # This function 'make_calendar_visualization_figure' is used to generate a visualization based on the provided data and parameters.\n",
        "  # 'days_not_nans' are the days, 'cluster_labels' are the cluster labels, 'n_clusters_t' is the number of clusters,\n",
        "  # '[2021]' represents the year, 'days_colors' represent the assigned colors for each day, 'color_to_cluster' maps colors to clusters,\n",
        "  # and 'save_figure' is an optional parameter to save the generated figure (can be None if not saving).\n",
        "\n",
        "  # Initialize empty lists to store centroid data\n",
        "  centroids_xx = []  # x-axis values for centroids\n",
        "  centroids_yy_daytypes = []  # y-axis values for centroids, grouped by day types\n",
        "  cluster_ids = []  # Cluster IDs\n",
        "\n",
        "  # Iterate through each cluster\n",
        "  for i in range(0, n_clusters_t):\n",
        "      # Store the x-axis values for centroids (hours of the day)\n",
        "      centroids_xx.append(x_axis_hours)\n",
        "\n",
        "      # Calculate the y-axis values for centroids (average flow for each 5-minute interval)\n",
        "      centroid_yy = list(np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).transpose())\n",
        "      centroids_yy_daytypes.append(centroid_yy)\n",
        "\n",
        "      # Store the cluster ID\n",
        "      cluster_ids.append(i)\n",
        "\n",
        "  # Generate a figure displaying the centroids\n",
        "  make_figure_centroids(centroids_xx, centroids_yy_daytypes, color_to_cluster, style_to_cluster, cluster_ids)\n",
        "  # The function 'make_figure_centroids' is used to create a visualization of the centroids,\n",
        "  # with the provided data and parameters.\n",
        "\n",
        "  # Import relevant metrics from scikit-learn\n",
        "  from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\n",
        "\n",
        "  # Calculate the Silhouette Score\n",
        "  SC_score = silhouette_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Silhouette Score measures the quality of clusters, higher values indicate better separation.\n",
        "\n",
        "  # Calculate the Davies-Bouldin Score\n",
        "  DB_score = davies_bouldin_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Davies-Bouldin Score measures the average similarity between each cluster and its most similar cluster, lower values indicate better separation.\n",
        "\n",
        "  # Calculate the Calinski-Harabasz Score\n",
        "  CH_score = calinski_harabasz_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "  # Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance, higher values indicate better separation.\n",
        "\n",
        "  # Print the computed cluster quality scores\n",
        "  print('Number of Clusters: ', n_clusters)\n",
        "  print('Silhouette Score:', SC_score)\n",
        "  print('Davies-Bouldin Score:', DB_score)\n",
        "  print('Calinski-Harabasz Score:', CH_score)\n",
        "  print('')\n",
        "\n",
        "    # Initialize a list to store centroid data\n",
        "  centroids = []\n",
        "\n",
        "  # Calculate centroids for each cluster\n",
        "  for i in np.unique(cluster_labels):\n",
        "      centroid = np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).reshape(1, nintvals)\n",
        "      centroids.append(centroid)\n",
        "\n",
        "  # Define the number of past intervals to consider for classification\n",
        "  n_past_intervals_for_classification = 5\n",
        "\n",
        "  # Initialize variables to calculate accuracy metrics\n",
        "  total_mae = 0\n",
        "  total_mape = 0\n",
        "  prediction_counts = 0\n",
        "\n",
        "  # Loop through each day in the evaluation dataset with no missing values\n",
        "  for i in range(0, ndays_eval_not_nans):\n",
        "      # Loop through intervals from n_past_intervals_for_classification to nintvals - 1\n",
        "      for j in range(n_past_intervals_for_classification, nintvals - 1):\n",
        "          # Find the closest centroid for the current data point\n",
        "          centroid_index = find_the_closest_centroid(centroids, vectorized_day_dataset_no_nans_eval[i].reshape(1, nintvals), j - n_past_intervals_for_classification, j)\n",
        "\n",
        "          # Predict the value for the next interval\n",
        "          predicted_value = centroids[centroid_index][0, j + 1]\n",
        "\n",
        "          # Calculate Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE)\n",
        "          mae_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "          mape_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1]) / float(vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "\n",
        "          # Accumulate MAE, MAPE, and count of predictions\n",
        "          total_mae += mae_t\n",
        "          total_mape += mape_t\n",
        "          prediction_counts += 1\n",
        "\n",
        "  # Calculate and print the prediction accuracy metrics\n",
        "  print('Prediction accuracy MAE:', total_mae / prediction_counts)\n",
        "  print('Prediction accuracy MAPE:', total_mape / prediction_counts)\n",
        "  print('')"
      ],
      "metadata": {
        "id": "TOG9QHFnZZRQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}